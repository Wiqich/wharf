package com.jesgoo.wharf.merger.puller

import java.util.List
import java.util.Properties
import java.util.UUID
import scala.collection.JavaConversions._
import scala.collection.mutable.HashMap
import org.apache.log4j.Logger
import com.jesgoo.wharf.core.config.LOG
import com.jesgoo.wharf.main.Merger
import com.jesgoo.wharf.thrift.wharfdata.Content
import com.jesgoo.wharf.thrift.wharfdata.Event
import kafka.message._
import kafka.producer.KeyedMessage
import kafka.producer.Producer
import kafka.producer.ProducerConfig
import java.io.File
import java.io.PrintWriter
import java.io.FileWriter

class KafkaPuller extends Puller {

  var topic: String = null;
  val brokerList: String = Merger.context.PULLER_KAFKA_BROKER_LIST
  /**
   * brokerList
   * This is for bootstrapping and the producer will only use it for getting metadata (topics, partitions and replicas).
   * The socket connections for sending the actual data will be established based on the broker information returned in
   * the metadata. The format is host1:port1,host2:port2, and the list can be a subset of brokers or a VIP pointing to a
   * subset of brokers.
   */
  val clientId: String = UUID.randomUUID().toString
  /**
   * clientId
   * The client id is a user-specified string sent in each request to help trace calls. It should logically identify
   * the application making the request.
   */
  val synchronously: Boolean = Merger.context.PULLER_KAFKA_SYNC
  /**
   * synchronously
   * This parameter specifies whether the messages are sent asynchronously in a background thread.
   * Valid values are false for asynchronous send and true for synchronous send. By setting the producer
   * to async we allow batching together of requests (which is great for throughput) but open the possibility
   * of a failure of the client machine dropping unsent data.
   */
  val compress: Boolean = Merger.context.PULLER_KAFKA_COMPRESS
  /**
   * compress
   * This parameter allows you to specify the compression codec for all data generated by this producer.
   * When set to true gzip is used.  To override and use snappy you need to implement that as the default
   * codec for compression using SnappyCompressionCodec.codec instead of DefaultCompressionCodec.codec below.
   */

  val batchSize: Integer = Merger.context.PULLER_KAFKA_BATCHSIZE
  /**
   * batchSize
   * The number of messages to send in one batch when using async mode.
   * The producer will wait until either this number of messages are ready
   * to send or queue.buffer.max.ms is reached.
   */
  val messageSendMaxRetries: Integer = Merger.context.PULLER_KAFKA_MAXRETRY
  /**
   * messageSendMaxRetries
   * This property will cause the producer to automatically retry a failed send request.
   * This property specifies the number of retries when such failures occur. Note that
   * setting a non-zero value here can lead to duplicates in the case of network errors
   * that cause a message to be sent but the acknowledgement to be lost.
   */
  val requestRequiredAcks: Integer = Merger.context.PULLER_KAFKA_REQUEST_ACKS
  /**
   * requestRequiredAcks
   *  0) which means that the producer never waits for an acknowledgement from the broker (the same behavior as 0.7).
   *     This option provides the lowest latency but the weakest durability guarantees (some data will be lost when a server fails).
   *  1) which means that the producer gets an acknowledgement after the leader replica has received the data. This option provides
   *     better durability as the client waits until the server acknowledges the request as successful (only messages that were
   *     written to the now-dead leader but not yet replicated will be lost).
   * -1) which means that the producer gets an acknowledgement after all in-sync replicas have received the data. This option
   *     provides the best durability, we guarantee that no messages will be lost as long as at least one in sync replica remains.
   */

  var producer: Producer[AnyRef, AnyRef] = null

  val period = Merger.context.PULLER_KAFKA_PERIOD

  val logger = Logger.getLogger(getClass.getName)
  val evetItems = new HashMap[String, Boolean]

  val store_status_path = Merger.context.PULLER_KAFKA_STATUS_DIR
  def pull(evt: Event): Boolean = {
    if (topic == null) {
      topic = evt.head.filename
    }
    if (producer == null) {
      init
    }
    val l_data: List[Content] = evt.body.getContents
    LOG.debug(logger, "Kafka put some data to=", topic, " data length=", String.valueOf(l_data.size()))
    var isflag = false
    for (i <- 0 to l_data.size() - 1) {
      if (!evetItems.contains(l_data.get(i).getMd5)) {
        isflag = send(l_data.get(i).getRel)
        if (!isflag) {
          return false
        } else {
          evetItems(l_data.get(i).getMd5) = true
        }
      }
    }
    LOG.debug(logger, "kafka evetItems size is", evetItems.size)
    store_status(evt)
    evetItems.clear()
    true
  }
  var count: Long = 0
  var status_writer: PrintWriter = null
  var flush_count = 0
  def store_status(event: Event) {
    try {
      if (status_writer == null) {
        val path = store_status_path + "/" + event.head.filename
        val path_file = path + "/" + event.getHead.getHostname + ".status"
        val p_file = new File(path)
        if (!p_file.exists()) {
          p_file.mkdirs()
        } 
        if (p_file.exists()){
          val status_file = new File(path_file)
          val op_file = new FileWriter(status_file, true)
          status_writer = new PrintWriter(op_file)
        }else{
          LOG.warn(logger, "Puller Kafka data mkdir dir error,path=", path)
        }
      }
      if (status_writer == null) {
        LOG.warn(logger, "Puller Kafka data writer init error")
      } else {
        status_writer.append(event.id + "," + count+"\n")
        count += 1
        flush_count +=1
        if (flush_count > Merger.context.PULLER_KAFKA_STATUS_FLUSH_LINES) {
          status_writer.flush()
          flush_count = 0
        }
      }
      if (count > 800000) {
        writer_close
        count = 0
      }
    } catch {
      case e: Exception =>
        e.printStackTrace()
        writer_close
    }
  }

  def writer_close() {
    try {
      if (status_writer != null) {
        status_writer.flush()
        status_writer.close()
      }
    } catch {
      case e: Exception =>
        e.printStackTrace()
    }
    status_writer = null
  }

  def init() {
    if (producer != null) {
      producer.close
    }
    evetItems.clear()
    val props = new Properties()

    val codec = if (compress) DefaultCompressionCodec.codec else NoCompressionCodec.codec

    props.put("compression.codec", codec.toString)
    props.put("producer.type", if (synchronously) "sync" else "async")
    props.put("metadata.broker.list", brokerList)
    props.put("batch.num.messages", batchSize.toString)
    props.put("message.send.max.retries", messageSendMaxRetries.toString)
    props.put("request.required.acks", requestRequiredAcks.toString)
    props.put("client.id", clientId.toString)
    producer = new Producer[AnyRef, AnyRef](new ProducerConfig(props))
  }
  def stop() {
    if (producer != null) {
      producer.close
    }
    isrun = false
  }

  var isrun = false
  def run() {
    mystatus = true
    isrun = true
    while (mystatus && isrun) {
      Thread.sleep(period)
    }
  }

  def kafkaMesssage(message: Array[Byte], partition: Array[Byte]): KeyedMessage[AnyRef, AnyRef] = {
    if (partition == null) {
      new KeyedMessage(topic, message)
    } else {
      new KeyedMessage(topic, partition, message)
    }
  }

  def send(message: String, partition: String = null): Boolean = send(message.getBytes("UTF8"), if (partition == null) null else partition.getBytes("UTF8"))

  def send(message: Array[Byte], partition: Array[Byte]): Boolean = {
    try {
      producer.send(kafkaMesssage(message, partition))
      true
    } catch {
      case e: Exception =>
        false
    }
  }
}